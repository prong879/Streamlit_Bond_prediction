"""
æ¨¡å‹è®­ç»ƒé¡µé¢
ç”¨äºé…ç½®å’Œè®­ç»ƒé¢„æµ‹æ¨¡å‹
"""
import streamlit as st
import pandas as pd
import numpy as np
import matplotlib.pyplot as plt
import torch
import torch.nn as nn
import statsmodels.api as sm
from statsmodels.stats.outliers_influence import variance_inflation_factor
from sklearn.feature_selection import SelectKBest, f_regression
from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns
import os
import json
from datetime import datetime
from pathlib import Path
import sys

# æ·»åŠ é¡¹ç›®æ ¹ç›®å½•åˆ°ç³»ç»Ÿè·¯å¾„
project_root = str(Path(__file__).parent.parent.parent)
if project_root not in sys.path:
    sys.path.append(project_root)

try:
    from web.utils.session import get_state, set_state, update_states
except ImportError:
    # å¦‚æœå¯¼å…¥å¤±è´¥ï¼Œåˆ›å»ºç©ºå‡½æ•°
    def get_state(key, default=None):
        return st.session_state.get(key, default)
    
    def set_state(key, value):
        st.session_state[key] = value
        
    def update_states(updates):
        for key, value in updates.items():
            st.session_state[key] = value

# ä¿®å¤PyTorchä¸Streamlitçš„å…¼å®¹æ€§é—®é¢˜
torch.classes.__path__ = []

from sklearn.preprocessing import MinMaxScaler
from sklearn.model_selection import train_test_split
import seaborn as sns
import os
import json
from datetime import datetime
from utils.session import get_state, set_state, update_states
from pathlib import Path
import sys

# é¡µé¢é…ç½®
st.set_page_config(
    page_title="æ¨¡å‹è®­ç»ƒ",
    page_icon="ğŸ§ ",
    layout="wide"
)

# æ ‡é¢˜å’Œç®€ä»‹
st.title("æ¨¡å‹è®­ç»ƒ")
st.markdown("æœ¬é¡µé¢ç”¨äºé…ç½®å’Œè®­ç»ƒæ—¶é—´åºåˆ—é¢„æµ‹æ¨¡å‹ã€‚é€‰æ‹©åˆé€‚çš„å‚æ•°å¹¶å¼€å§‹è®­ç»ƒè¿‡ç¨‹ã€‚")

# è·å–åŠ è½½çš„æ•°æ®
if 'raw_data' not in st.session_state:
    st.warning("è¯·å…ˆåœ¨æ•°æ®æŸ¥çœ‹é¡µé¢åŠ è½½æ•°æ®")
    st.stop()

df = st.session_state['raw_data']
tech_indicators = None

# åˆ›å»ºä¸‰æ å¸ƒå±€
left_column, middle_column, right_column = st.columns([1, 2, 1])

# å·¦ä¾§æ  - æ•°æ®ä¿¡æ¯å’Œç‰¹å¾é€‰æ‹©
with left_column:
    st.subheader("æ•°æ®å’Œç‰¹å¾")
    
    # æ˜¾ç¤ºæ•°æ®åŸºæœ¬ä¿¡æ¯
    with st.expander("æ•°æ®ä¿¡æ¯", expanded=True):
        if 'raw_data' in st.session_state:
            df = st.session_state['raw_data']
            st.write(f"æ•°æ®å½¢çŠ¶: {df.shape}")
            st.write(f"æ—¶é—´èŒƒå›´: {df.index.min()} è‡³ {df.index.max()}")
    
    # ç‰¹å¾é€‰æ‹©
    with st.expander("ç‰¹å¾é€‰æ‹©", expanded=True):
        if 'raw_data' in st.session_state:
            df = st.session_state['raw_data']
            all_features = df.columns.tolist()
            selected_features = st.multiselect(
                "é€‰æ‹©ç”¨äºè®­ç»ƒçš„ç‰¹å¾",
                options=all_features,
                default=['Close'] if 'Close' in all_features else all_features[:1]
            )
    
    # æ•°æ®åˆ’åˆ†è®¾ç½®        
    with st.expander("æ•°æ®åˆ’åˆ†", expanded=True):
        train_test_ratio = st.slider(
            "è®­ç»ƒé›†æ¯”ä¾‹", 
            min_value=0.5, 
            max_value=0.9, 
            value=0.8, 
            step=0.05,
            help="è®­ç»ƒé›†å æ€»æ•°æ®çš„æ¯”ä¾‹"
        )
        
        sequence_length = st.number_input(
            "è¾“å…¥åºåˆ—é•¿åº¦",
            min_value=1,
            max_value=100,
            value=10,
            help="ç”¨äºé¢„æµ‹çš„å†å²æ•°æ®ç‚¹æ•°é‡"
        )
        
        prediction_length = st.number_input(
            "é¢„æµ‹åºåˆ—é•¿åº¦",
            min_value=1,
            max_value=30,
            value=1,
            help="éœ€è¦é¢„æµ‹çš„æœªæ¥æ•°æ®ç‚¹æ•°é‡"
        )

# ä¸­é—´æ  - æ¨¡å‹å‚æ•°è®¾ç½®ä¸è®­ç»ƒæ§åˆ¶
with middle_column:
    st.subheader("æ¨¡å‹å‚æ•°é…ç½®")
    
    # æ¨¡å‹ç±»å‹é€‰æ‹©æ ‡ç­¾é¡µ
    model_tabs = st.tabs(["LSTM", "ARIMA", "Prophet"])
    
    # LSTMå‚æ•°è®¾ç½®
    with model_tabs[0]:
        st.markdown("### LSTMæ¨¡å‹å‚æ•°")
        
        col1, col2 = st.columns(2)
        with col1:
            hidden_size = st.number_input(
                "éšè—å±‚å¤§å°",
                min_value=1,
                max_value=512,
                value=64
            )
            
            num_layers = st.number_input(
                "LSTMå±‚æ•°",
                min_value=1,
                max_value=5,
                value=2
            )
            
            dropout = st.slider(
                "Dropoutæ¯”ä¾‹",
                min_value=0.0,
                max_value=0.9,
                value=0.2,
                step=0.1
            )
        
        with col2:
            learning_rate = st.number_input(
                "å­¦ä¹ ç‡",
                min_value=0.0001,
                max_value=0.1,
                value=0.001,
                format="%.4f"
            )
            
            batch_size = st.number_input(
                "æ‰¹æ¬¡å¤§å°",
                min_value=1,
                max_value=256,
                value=32
            )
            
            epochs = st.number_input(
                "è®­ç»ƒè½®æ•°",
                min_value=1,
                max_value=1000,
                value=100
            )
    
    # ARIMAå‚æ•°è®¾ç½®
    with model_tabs[1]:
        st.markdown("### ARIMAæ¨¡å‹å‚æ•°")
        
        col1, col2 = st.columns(2)
        with col1:
            p_param = st.number_input(
                "p (ARé˜¶æ•°)",
                min_value=0,
                max_value=10,
                value=2
            )
            
            d_param = st.number_input(
                "d (å·®åˆ†é˜¶æ•°)",
                min_value=0,
                max_value=2,
                value=1
            )
        
        with col2:
            q_param = st.number_input(
                "q (MAé˜¶æ•°)",
                min_value=0,
                max_value=10,
                value=2
            )
            
            seasonal = st.checkbox(
                "åŒ…å«å­£èŠ‚æ€§æˆåˆ†",
                value=False
            )
    
    # Prophetå‚æ•°è®¾ç½®
    with model_tabs[2]:
        st.markdown("### Prophetæ¨¡å‹å‚æ•°")
        
        col1, col2 = st.columns(2)
        with col1:
            yearly_seasonality = st.selectbox(
                "å¹´åº¦å­£èŠ‚æ€§",
                options=["auto", "True", "False"],
                index=0
            )
            
            weekly_seasonality = st.selectbox(
                "å‘¨åº¦å­£èŠ‚æ€§",
                options=["auto", "True", "False"],
                index=0
            )
        
        with col2:
            daily_seasonality = st.selectbox(
                "æ—¥åº¦å­£èŠ‚æ€§",
                options=["auto", "True", "False"],
                index=0
            )
            
            changepoint_prior_scale = st.slider(
                "å˜ç‚¹å…ˆéªŒæ¯”ä¾‹",
                min_value=0.001,
                max_value=0.5,
                value=0.05,
                step=0.001,
                format="%.3f"
            )
    

    
    # è®­ç»ƒæ§åˆ¶
    st.markdown("### è®­ç»ƒæ§åˆ¶")
    
    train_col1, train_col2 = st.columns([3, 1])
    with train_col1:
        start_training = st.button(
            "å¼€å§‹è®­ç»ƒ",
            use_container_width=True
        )
        
    with train_col2:
        enable_early_stopping = st.checkbox(
            "å¯ç”¨æ—©åœ",
            value=True
        )
    
    # è®­ç»ƒè¿›åº¦å’ŒæŸå¤±å¯è§†åŒ–çš„å ä½åŒºåŸŸ
    progress_placeholder = st.empty()
    loss_chart_placeholder = st.empty()
    
    # å¦‚æœä¼šè¯ä¸­å·²æœ‰è®­ç»ƒå†å²ä½†ç•Œé¢åˆšåˆšåŠ è½½ï¼Œæ˜¾ç¤ºä¹‹å‰çš„è®­ç»ƒå†å²
    if 'training_history' in st.session_state and 'training_complete' in st.session_state and st.session_state['training_complete'] and not start_training:
        history = st.session_state['training_history']
        with loss_chart_placeholder:
            # ç»˜åˆ¶å·²æœ‰çš„æŸå¤±æ›²çº¿
            history_df = pd.DataFrame({
                'è®­ç»ƒæŸå¤±': history['train_loss'],
                'éªŒè¯æŸå¤±': history['val_loss']
            })
            st.line_chart(history_df)
    
    if start_training:
        with progress_placeholder.container():
            st.info("è®­ç»ƒè¿‡ç¨‹å°†åœ¨è¿™é‡Œæ˜¾ç¤º...")
            progress_bar = st.progress(0)
            status_text = st.empty()
            
        with loss_chart_placeholder.container():
            # ä¸´æ—¶æ•°æ®ç”¨äºç¤ºä¾‹
            chart_data = pd.DataFrame(
                np.random.randn(20, 2),
                columns=['è®­ç»ƒæŸå¤±', 'éªŒè¯æŸå¤±']
            )
            st.line_chart(chart_data)
    
# å³ä¾§æ  - æ¨¡å‹ä¿å­˜å’Œä¿¡æ¯æ˜¾ç¤º
with right_column:
    st.subheader("æ¨¡å‹ä¿¡æ¯")
    
    # æ¨¡å‹çŠ¶æ€ä¿¡æ¯
    with st.expander("è®­ç»ƒçŠ¶æ€", expanded=True):
        if 'training_complete' in st.session_state and st.session_state['training_complete']:
            st.success("æ¨¡å‹è®­ç»ƒå·²å®Œæˆ")
        elif start_training:
            st.info("æ¨¡å‹è®­ç»ƒä¸­...")
        else:
            st.info("ç­‰å¾…å¼€å§‹è®­ç»ƒ...")
    
    # æ¨¡å‹ä¿å­˜é€‰é¡¹
    with st.expander("æ¨¡å‹ä¿å­˜", expanded=True):
        model_name = st.text_input(
            "æ¨¡å‹åç§°",
            value="my_model_v1"
        )
        
        save_model_button = st.button(
            "ä¿å­˜æ¨¡å‹",
            disabled=not ('training_complete' in st.session_state and st.session_state['training_complete'])
        )
        
        if save_model_button and 'trained_model' in st.session_state:
            model_path = save_model(
                st.session_state['trained_model'],
                st.session_state['model_params'],
                st.session_state['training_params'],
                st.session_state['training_history'],
                path=f"models/{model_name}"
            )
            st.success(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
    
    # æ¨¡å‹è¯„ä¼°ç®€æŠ¥
    with st.expander("æ¨¡å‹è¯„ä¼°ç®€æŠ¥", expanded=True):
        if 'model_metrics' in st.session_state and st.session_state.get('model_metrics') is not None:
            metrics = st.session_state['model_metrics']
            st.metric(
                label="MSE",
                value=f"{metrics.get('MSE', 0):.4f}"
            )
            
            st.metric(
                label="RMSE",
                value=f"{metrics.get('RMSE', 0):.4f}"
            )
            
            st.metric(
                label="MAE",
                value=f"{metrics.get('MAE', 0):.4f}"
            )
        elif start_training:
            st.info("æ¨¡å‹è¯„ä¼°ä¸­...")
        else:
            st.info("è®­ç»ƒæ¨¡å‹åå°†æ˜¾ç¤ºè¯„ä¼°æŒ‡æ ‡")

# å®šä¹‰LSTMæ¨¡å‹
class LSTMModel(nn.Module):
    """
    LSTMæ¨¡å‹å®šä¹‰
    
    Args:
        input_dim: è¾“å…¥ç‰¹å¾ç»´åº¦
        hidden_dim: éšè—å±‚ç»´åº¦
        num_layers: LSTMå±‚æ•°
        output_dim: è¾“å‡ºç»´åº¦
        dropout: Dropoutæ¯”ç‡
    """
    def __init__(self, input_dim, hidden_dim, num_layers, output_dim, dropout=0.2):
        super(LSTMModel, self).__init__()
        self.hidden_dim = hidden_dim
        self.num_layers = num_layers
        
        # LSTMå±‚
        self.lstm = nn.LSTM(
            input_size=input_dim,
            hidden_size=hidden_dim,
            num_layers=num_layers,
            batch_first=True,
            dropout=dropout if num_layers > 1 else 0
        )
        
        # å…¨è¿æ¥è¾“å‡ºå±‚
        self.fc = nn.Linear(hidden_dim, output_dim)
        
    def forward(self, x):
        # åˆå§‹åŒ–éšè—çŠ¶æ€
        h0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        c0 = torch.zeros(self.num_layers, x.size(0), self.hidden_dim).to(x.device)
        
        # å‰å‘ä¼ æ’­LSTM
        out, _ = self.lstm(x, (h0, c0))
        
        # å–æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„è¾“å‡º
        out = self.fc(out[:, -1, :])
        return out

def create_sequences(data, seq_length):
    """
    åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
    
    Args:
        data: è¾“å…¥æ•°æ®
        seq_length: åºåˆ—é•¿åº¦
        
    Returns:
        X: ç‰¹å¾åºåˆ—
        y: ç›®æ ‡å€¼
    """
    xs, ys = [], []
    for i in range(len(data) - seq_length):
        x = data[i:(i + seq_length)]
        y = data[i + seq_length]
        xs.append(x)
        ys.append(y)
    return np.array(xs), np.array(ys)

def train_lstm_model(X_train, y_train, X_val, y_val, model_params, training_params, progress_bar=None, status_text=None, loss_chart=None):
    """
    è®­ç»ƒLSTMæ¨¡å‹
    
    Args:
        X_train: è®­ç»ƒç‰¹å¾æ•°æ®
        y_train: è®­ç»ƒç›®æ ‡æ•°æ®
        X_val: éªŒè¯ç‰¹å¾æ•°æ®
        y_val: éªŒè¯ç›®æ ‡æ•°æ®
        model_params: æ¨¡å‹å‚æ•°å­—å…¸
        training_params: è®­ç»ƒå‚æ•°å­—å…¸
        progress_bar: streamlitè¿›åº¦æ¡
        status_text: streamlitçŠ¶æ€æ–‡æœ¬
        loss_chart: æŸå¤±æ›²çº¿å›¾è¡¨å ä½ç¬¦
        
    Returns:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        history: è®­ç»ƒå†å²
    """
    # è½¬æ¢ä¸ºPyTorchå¼ é‡
    X_train_tensor = torch.FloatTensor(X_train)
    y_train_tensor = torch.FloatTensor(y_train)
    X_val_tensor = torch.FloatTensor(X_val)
    y_val_tensor = torch.FloatTensor(y_val)
    
    # åˆ›å»ºæ¨¡å‹
    model = LSTMModel(
        input_dim=model_params['input_dim'],
        hidden_dim=model_params['hidden_dim'],
        num_layers=model_params['num_layers'],
        output_dim=model_params['output_dim'],
        dropout=model_params.get('dropout', 0.2)
    )
    
    # å®šä¹‰æŸå¤±å‡½æ•°å’Œä¼˜åŒ–å™¨
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=training_params['learning_rate'])
    
    # è®­ç»ƒå‚æ•°
    epochs = training_params['epochs']
    batch_size = training_params['batch_size']
    
    # è®­ç»ƒå†å²è®°å½•
    history = {
        'train_loss': [],
        'val_loss': []
    }
    
    # ä½¿ç”¨ä¼ å…¥çš„è¿›åº¦æ¡æˆ–åˆ›å»ºæ–°çš„
    if progress_bar is None:
        progress_bar = st.progress(0)
    if status_text is None:
        status_text = st.empty()
        
    # åˆ›å»ºæŸå¤±å›¾è¡¨çš„DataFrame
    loss_df = pd.DataFrame(columns=['è®­ç»ƒæŸå¤±', 'éªŒè¯æŸå¤±'])
    
    # è®­ç»ƒå¾ªç¯
    for epoch in range(epochs):
        model.train()
        train_losses = []
        
        # å°æ‰¹é‡è®­ç»ƒ
        for i in range(0, len(X_train_tensor), batch_size):
            batch_X = X_train_tensor[i:i+batch_size]
            batch_y = y_train_tensor[i:i+batch_size]
            
            # å‰å‘ä¼ æ’­
            outputs = model(batch_X)
            loss = criterion(outputs, batch_y)
            
            # åå‘ä¼ æ’­å’Œä¼˜åŒ–
            optimizer.zero_grad()
            loss.backward()
            optimizer.step()
            
            train_losses.append(loss.item())
        
        # éªŒè¯
        model.eval()
        with torch.no_grad():
            val_outputs = model(X_val_tensor)
            val_loss = criterion(val_outputs, y_val_tensor)
            
        # è®°å½•è®­ç»ƒå’ŒéªŒè¯æŸå¤±
        avg_train_loss = sum(train_losses) / len(train_losses)
        history['train_loss'].append(avg_train_loss)
        history['val_loss'].append(val_loss.item())
        
        # æ›´æ–°æŸå¤±å›¾è¡¨
        loss_df.loc[epoch] = [avg_train_loss, val_loss.item()]
        if loss_chart is not None:
            with loss_chart:
                st.line_chart(loss_df)
        
        # æ›´æ–°è¿›åº¦æ¡å’ŒçŠ¶æ€æ–‡æœ¬
        progress = (epoch + 1) / epochs
        progress_bar.progress(progress)
        status_text.text(f"Epoch {epoch+1}/{epochs}, è®­ç»ƒæŸå¤±: {avg_train_loss:.6f}, éªŒè¯æŸå¤±: {val_loss.item():.6f}")
    
    progress_bar.empty()
    status_text.text("æ¨¡å‹è®­ç»ƒå®Œæˆï¼")
    
    return model, history

def plot_training_history(history, chart_placeholder=None):
    """
    ç»˜åˆ¶è®­ç»ƒå†å²
    
    Args:
        history: è®­ç»ƒå†å²å­—å…¸
        chart_placeholder: å›¾è¡¨å ä½ç¬¦
    """
    # åˆ›å»ºDataFrameç”¨äºç»˜å›¾
    history_df = pd.DataFrame({
        'è®­ç»ƒæŸå¤±': history['train_loss'],
        'éªŒè¯æŸå¤±': history['val_loss']
    })
    
    if chart_placeholder is not None:
        with chart_placeholder:
            st.line_chart(history_df)
    else:
        st.line_chart(history_df)

def save_model(model, model_params, training_params, history, path="models"):
    """
    ä¿å­˜æ¨¡å‹å’Œè®­ç»ƒå‚æ•°
    
    Args:
        model: è®­ç»ƒå¥½çš„æ¨¡å‹
        model_params: æ¨¡å‹å‚æ•°
        training_params: è®­ç»ƒå‚æ•°
        history: è®­ç»ƒå†å²
        path: ä¿å­˜è·¯å¾„
    
    Returns:
        model_path: æ¨¡å‹ä¿å­˜è·¯å¾„
    """
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    os.makedirs(path, exist_ok=True)
    
    # ç”Ÿæˆæ—¶é—´æˆ³
    timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
    
    # ä¿å­˜æ¨¡å‹
    model_filename = f"lstm_model_{timestamp}.pth"
    model_path = os.path.join(path, model_filename)
    torch.save(model.state_dict(), model_path)
    
    # ä¿å­˜æ¨¡å‹å‚æ•°å’Œè®­ç»ƒå†å²
    params_filename = f"model_params_{timestamp}.json"
    params_path = os.path.join(path, params_filename)
    
    params_dict = {
        'model_params': model_params,
        'training_params': training_params,
        'training_history': {
            'train_loss': [float(loss) for loss in history['train_loss']],
            'val_loss': [float(loss) for loss in history['val_loss']]
        },
        'timestamp': timestamp
    }
    
    with open(params_path, 'w') as f:
        json.dump(params_dict, f, indent=2)
    
    return model_path

# ç”¨äºåœ¨ä¼šè¯é—´ä¿å­˜æ¨¡å‹è®­ç»ƒçŠ¶æ€
if 'trained_models' not in st.session_state:
    st.session_state['trained_models'] = {}

# ç”¨äºä¿å­˜æ¨¡å‹è®­ç»ƒå†å²è®°å½•
if 'training_history' not in st.session_state:
    st.session_state['training_history'] = {}

# é¡µé¢åº•éƒ¨ - å¸®åŠ©ä¿¡æ¯
with st.expander("ä½¿ç”¨å¸®åŠ©"):
    st.markdown("""
    ### ä½¿ç”¨è¯´æ˜
    
    1. **æ•°æ®å‡†å¤‡**: åœ¨æ•°æ®æŸ¥çœ‹é¡µé¢ä¸Šä¼ å¹¶å¤„ç†æ‚¨çš„æ•°æ®
    2. **ç‰¹å¾é€‰æ‹©**: é€‰æ‹©ç”¨äºè®­ç»ƒæ¨¡å‹çš„ç‰¹å¾
    3. **æ¨¡å‹å‚æ•°**: é…ç½®æ¨¡å‹çš„è¶…å‚æ•°
    4. **å¼€å§‹è®­ç»ƒ**: ç‚¹å‡»"å¼€å§‹è®­ç»ƒ"æŒ‰é’®å¯åŠ¨è®­ç»ƒè¿‡ç¨‹
    5. **ä¿å­˜æ¨¡å‹**: è®­ç»ƒå®Œæˆåï¼Œå¯ä»¥ä¿å­˜æ¨¡å‹ä»¥ä¾¿åç»­ä½¿ç”¨
    
    ### å‚æ•°è§£é‡Š
    
    #### LSTMå‚æ•°
    - **éšè—å±‚å¤§å°**: ç¥ç»ç½‘ç»œéšè—å±‚çš„èŠ‚ç‚¹æ•°é‡
    - **LSTMå±‚æ•°**: æ¨¡å‹ä¸­LSTMå±‚çš„æ•°é‡
    - **Dropoutæ¯”ä¾‹**: é˜²æ­¢è¿‡æ‹Ÿåˆçš„éšæœºä¸¢å¼ƒæ¯”ä¾‹
    - **å­¦ä¹ ç‡**: æ¢¯åº¦ä¸‹é™çš„æ­¥é•¿
    - **æ‰¹æ¬¡å¤§å°**: æ¯æ¬¡æ›´æ–°æƒé‡ä½¿ç”¨çš„æ ·æœ¬æ•°é‡
    - **è®­ç»ƒè½®æ•°**: å®Œæ•´æ•°æ®é›†çš„è®­ç»ƒæ¬¡æ•°
    
    #### ARIMAå‚æ•°
    - **p (ARé˜¶æ•°)**: è‡ªå›å½’é¡¹çš„é˜¶æ•°
    - **d (å·®åˆ†é˜¶æ•°)**: å·®åˆ†é˜¶æ•°ï¼Œä½¿åºåˆ—å¹³ç¨³
    - **q (MAé˜¶æ•°)**: ç§»åŠ¨å¹³å‡é¡¹çš„é˜¶æ•°
    """)

# å®é™…æ‰§è¡Œè®­ç»ƒçš„é€»è¾‘
if start_training:
    # å‡†å¤‡ç‰¹å¾æ•°æ®
    if not selected_features:
        st.error("è¯·è‡³å°‘é€‰æ‹©ä¸€ä¸ªç‰¹å¾ç”¨äºè®­ç»ƒ")
        st.stop()
    
    with progress_placeholder.container():
        st.info("å‡†å¤‡è®­ç»ƒæ•°æ®...")
        progress_bar = st.progress(0)
        status_text = st.empty()
        
        # æå–é€‰å®šçš„ç‰¹å¾
        feature_data = df[selected_features].values
        target_data = df['Close'].values.reshape(-1, 1) if 'Close' in df.columns else df[df.columns[0]].values.reshape(-1, 1)
        
        # æ•°æ®å½’ä¸€åŒ–
        feature_scaler = MinMaxScaler()
        target_scaler = MinMaxScaler()
        
        feature_data = feature_scaler.fit_transform(feature_data)
        target_data = target_scaler.fit_transform(target_data)
        
        # ä¿å­˜å½’ä¸€åŒ–å™¨ä»¥ä¾›åç»­é¢„æµ‹ä½¿ç”¨
        st.session_state['feature_scaler'] = feature_scaler
        st.session_state['target_scaler'] = target_scaler
        
        # åˆ›å»ºæ—¶é—´åºåˆ—æ•°æ®
        X, y = create_sequences(
            np.column_stack((feature_data, target_data)), 
            int(sequence_length)
        )
        
        # åˆ†ç¦»ç›®æ ‡å˜é‡
        X = X[:, :, :-1]  # ç§»é™¤æœ€åä¸€åˆ—ï¼ˆç›®æ ‡å˜é‡ï¼‰
        y = y[:, -1:]     # åªå–æœ€åä¸€åˆ—ï¼ˆç›®æ ‡å˜é‡ï¼‰
        
        # åˆ’åˆ†è®­ç»ƒã€éªŒè¯å’Œæµ‹è¯•é›†
        total_samples = len(X)
        train_size = int(total_samples * train_test_ratio)
        val_size = int(total_samples * 0.15)  # å›ºå®š15%çš„éªŒè¯é›†
        
        X_train, y_train = X[:train_size], y[:train_size]
        X_val, y_val = X[train_size:train_size+val_size], y[train_size:train_size+val_size]
        X_test, y_test = X[train_size+val_size:], y[train_size+val_size:]
        
        st.info(f"è®­ç»ƒé›†å¤§å°: {X_train.shape[0]}, éªŒè¯é›†å¤§å°: {X_val.shape[0]}, æµ‹è¯•é›†å¤§å°: {X_test.shape[0]}")
        
        # è®¾ç½®æ¨¡å‹å‚æ•°
        model_params = {
            'input_dim': X_train.shape[2],  # ç‰¹å¾ç»´åº¦
            'hidden_dim': hidden_size,
            'num_layers': num_layers,
            'output_dim': y_train.shape[1],  # è¾“å‡ºç»´åº¦
            'dropout': dropout
        }
        
        # è®¾ç½®è®­ç»ƒå‚æ•°
        training_params = {
            'learning_rate': learning_rate,
            'batch_size': batch_size,
            'epochs': epochs
        }
        
        # è®­ç»ƒæ¨¡å‹
        st.info("å¼€å§‹è®­ç»ƒLSTMæ¨¡å‹...")
        model, history = train_lstm_model(X_train, y_train, X_val, y_val, model_params, training_params, 
                                         progress_bar=progress_bar, status_text=status_text, 
                                         loss_chart=loss_chart_placeholder)
        
        # ç»˜åˆ¶è®­ç»ƒå†å²
        plot_training_history(history, loss_chart_placeholder)
        
        # æµ‹è¯•é›†è¯„ä¼°
        st.subheader("æ¨¡å‹è¯„ä¼°")
        model.eval()
        with torch.no_grad():
            X_test_tensor = torch.FloatTensor(X_test)
            y_test_tensor = torch.FloatTensor(y_test)
            
            test_outputs = model(X_test_tensor)
            test_loss = nn.MSELoss()(test_outputs, y_test_tensor)
            
            # åå½’ä¸€åŒ–é¢„æµ‹ç»“æœç”¨äºå±•ç¤º
            test_predictions = target_scaler.inverse_transform(test_outputs.numpy())
            test_actual = target_scaler.inverse_transform(y_test)
            
            # è®¡ç®—è¯„ä¼°æŒ‡æ ‡
            mse = np.mean((test_predictions - test_actual) ** 2)
            rmse = np.sqrt(mse)
            mae = np.mean(np.abs(test_predictions - test_actual))
            
            # æ›´æ–°å³ä¾§æ ä¸­çš„è¯„ä¼°æŒ‡æ ‡
            st.session_state['model_metrics'] = {
                'MSE': float(mse),
                'RMSE': float(rmse),
                'MAE': float(mae)
            }
        
        # ä¿å­˜æ¨¡å‹
        model_path = save_model(model, model_params, training_params, history)
        st.success(f"æ¨¡å‹å·²ä¿å­˜åˆ°: {model_path}")
        
        # æ›´æ–°ä¼šè¯çŠ¶æ€
        st.session_state['trained_model'] = model
        st.session_state['model_params'] = model_params
        st.session_state['training_params'] = training_params
        st.session_state['training_history'] = history
        st.session_state['X_test'] = X_test
        st.session_state['y_test'] = y_test
        st.session_state['seq_length'] = sequence_length
        
        # æ›´æ–°è®­ç»ƒçŠ¶æ€
        st.session_state['training_complete'] = True
        
        # åˆ·æ–°å³ä¾§æ çŠ¶æ€æ˜¾ç¤º
        with right_column:
            with st.expander("è®­ç»ƒçŠ¶æ€", expanded=True):
                st.success("æ¨¡å‹è®­ç»ƒå·²å®Œæˆ")

def select_features(df, correlation_threshold=0.5, vif_threshold=10, p_value_threshold=0.05):
    """
    åŸºäºç›¸å…³æ€§ã€å¤šé‡å…±çº¿æ€§å’Œç»Ÿè®¡æ˜¾è‘—æ€§è¿›è¡Œç‰¹å¾é€‰æ‹©
    
    å‚æ•°:
    df: åŒ…å«ç‰¹å¾çš„DataFrame
    correlation_threshold: ç›¸å…³æ€§é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.5
    vif_threshold: VIFé˜ˆå€¼ï¼Œé»˜è®¤ä¸º10
    p_value_threshold: på€¼é˜ˆå€¼ï¼Œé»˜è®¤ä¸º0.05
    
    è¿”å›:
    selected_features: é€‰æ‹©çš„ç‰¹å¾åˆ—è¡¨
    """
    try:
        target_col = 'Close'
        # ç¡®ä¿ç›®æ ‡å˜é‡å­˜åœ¨äºæ•°æ®é›†ä¸­
        if target_col not in df.columns:
            st.warning(f"ç›®æ ‡å˜é‡ '{target_col}' ä¸åœ¨æ•°æ®é›†ä¸­ã€‚å°†ä½¿ç”¨ç¬¬ä¸€åˆ—ä½œä¸ºç›®æ ‡å˜é‡ã€‚")
            target_col = df.columns[0]
            
        # ä½¿ç”¨æ•°å€¼å‹åˆ—è¿›è¡Œåˆ†æ
        numeric_cols = df.select_dtypes(include=[np.number]).columns.tolist()
        if target_col not in numeric_cols:
            st.warning(f"ç›®æ ‡å˜é‡ '{target_col}' ä¸æ˜¯æ•°å€¼ç±»å‹ã€‚ç‰¹å¾é€‰æ‹©å¯èƒ½æ— æ³•æ­£å¸¸å·¥ä½œã€‚")
        
        selected_features = []
        
        # æ­¥éª¤1: åŸºäºç›¸å…³æ€§çš„ç‰¹å¾é€‰æ‹©
        # è®¡ç®—ä¸ç›®æ ‡å˜é‡(Close)çš„ç›¸å…³æ€§
        correlation_matrix = df[numeric_cols].corr(numeric_only=True)
        target_correlations = correlation_matrix[target_col].sort_values(ascending=False)
        
        # æ˜¾ç¤ºç›¸å…³æ€§æ’å
        with st.expander("**ç›¸å…³æ€§ç­›é€‰**", expanded=False):
            corr_df = pd.DataFrame({
                'ç‰¹å¾': target_correlations.index,
                'ç›¸å…³æ€§': target_correlations.values
            })
            st.dataframe(corr_df)
            # é€‰æ‹©ç›¸å…³æ€§é«˜äºé˜ˆå€¼çš„ç‰¹å¾
            high_correlation_features = target_correlations[abs(target_correlations) > correlation_threshold].index.tolist()
            st.write(f"ç›¸å…³æ€§é«˜äº{correlation_threshold}çš„ç‰¹å¾: {high_correlation_features}")
        
        # æ­¥éª¤2: å¤šé‡å…±çº¿æ€§åˆ†æ - è®¡ç®—VIF (Variance Inflation Factor)
        # åˆ›å»ºä¸€ä¸ªæ²¡æœ‰ç›®æ ‡å˜é‡çš„ç‰¹å¾å­é›†
        X = df[numeric_cols].drop(target_col, axis=1)
        
        # æ£€æŸ¥æ˜¯å¦æœ‰è¶³å¤Ÿçš„æ ·æœ¬
        if len(X) <= len(X.columns):
            st.warning("æ ·æœ¬æ•°é‡ä¸è¶³ä»¥è¿›è¡Œå¤šé‡å…±çº¿æ€§åˆ†æã€‚è·³è¿‡VIFè®¡ç®—ã€‚")
            high_vif_features = []
        else:
            try:
                # æ·»åŠ å¸¸æ•°é¡¹
                X_with_const = sm.add_constant(X)
                
                # è®¡ç®—VIF
                vif_data = pd.DataFrame()
                vif_data["ç‰¹å¾"] = X_with_const.columns
                
                # å®‰å…¨åœ°è®¡ç®—VIFï¼Œå¤„ç†å¯èƒ½çš„é”™è¯¯
                vif_values = []
                for i in range(X_with_const.shape[1]):
                    try:
                        vif_i = variance_inflation_factor(X_with_const.values, i)
                        vif_values.append(vif_i)
                    except Exception as e:
                        st.warning(f"è®¡ç®—ç‰¹å¾ '{X_with_const.columns[i]}' çš„VIFæ—¶å‡ºé”™: {str(e)}")
                        vif_values.append(float('inf'))  # æ ‡è®°ä¸ºæ— ç©·å¤§
                
                vif_data["VIF"] = vif_values
                vif_data = vif_data.sort_values("VIF", ascending=False)
                
                # æ˜¾ç¤ºVIFåˆ†æç»“æœ
                with st.expander("**å¤šé‡å…±çº¿æ€§åˆ†æ**", expanded=False):
                    st.dataframe(vif_data)
                    st.info("VIF > 10è¡¨ç¤ºå­˜åœ¨ä¸¥é‡çš„å¤šé‡å…±çº¿æ€§")
                
                # ç§»é™¤VIFè¿‡é«˜çš„ç‰¹å¾
                high_vif_features = vif_data[vif_data["VIF"] > vif_threshold]["ç‰¹å¾"].tolist()
                if 'const' in high_vif_features:
                    high_vif_features.remove('const')  # ç§»é™¤å¸¸æ•°é¡¹
                
                with st.expander("**å¤šé‡å…±çº¿æ€§è¿‡æ»¤**", expanded=False):
                    st.write(f"å¤šé‡å…±çº¿æ€§ä¸¥é‡çš„ç‰¹å¾ (VIF > {vif_threshold}): {high_vif_features}")
            except Exception as e:
                st.warning(f"VIFè®¡ç®—å¤±è´¥: {str(e)}")
                high_vif_features = []
        
        # æ­¥éª¤3: åŸºäºç»Ÿè®¡æ˜¾è‘—æ€§çš„ç‰¹å¾é€‰æ‹©
        try:
            # ä½¿ç”¨f_regressionè¯„ä¼°ç‰¹å¾çš„ç»Ÿè®¡æ˜¾è‘—æ€§
            X = df[numeric_cols].drop(target_col, axis=1).values
            y = df[target_col].values
            
            # ç¡®ä¿Xå’Œyæœ‰ç›¸åŒçš„æ ·æœ¬æ•°
            if len(X) != len(y):
                st.warning("ç‰¹å¾çŸ©é˜µå’Œç›®æ ‡å˜é‡é•¿åº¦ä¸åŒ¹é…ï¼Œæ— æ³•è¿›è¡Œæ˜¾è‘—æ€§åˆ†æ")
                significant_features = []
            else:
                f_selector = SelectKBest(f_regression, k='all')
                f_selector.fit(X, y)
                
                # è·å–æ¯ä¸ªç‰¹å¾çš„på€¼å’ŒFå€¼
                f_scores = pd.DataFrame()
                f_scores["ç‰¹å¾"] = df[numeric_cols].drop(target_col, axis=1).columns
                f_scores["Fç»Ÿè®¡é‡"] = f_selector.scores_
                f_scores["På€¼"] = f_selector.pvalues_
                f_scores = f_scores.sort_values("Fç»Ÿè®¡é‡", ascending=False)
                
                with st.expander("**ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æ**", expanded=False):
                    st.dataframe(f_scores)
                    st.info("På€¼ < 0.05 è¡¨ç¤ºç‰¹å¾å…·æœ‰ç»Ÿè®¡æ˜¾è‘—æ€§")
                
                # é€‰æ‹©ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾(på€¼<p_value_threshold)
                significant_features = f_scores[f_scores["På€¼"] < p_value_threshold]["ç‰¹å¾"].tolist()
                
                with st.expander("**ç»Ÿè®¡æ˜¾è‘—æ€§è¿‡æ»¤**", expanded=False):
                    st.write(f"ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾ (P < {p_value_threshold}): {significant_features}")
        except Exception as e:
            st.warning(f"ç»Ÿè®¡æ˜¾è‘—æ€§åˆ†æå¤±è´¥: {str(e)}")
            significant_features = []
        
        # æ­¥éª¤4: ç»¼åˆåˆ†æï¼Œé€‰æ‹©æœ€ç»ˆçš„ç‰¹å¾é›†
        # ä»é«˜ç›¸å…³æ€§ç‰¹å¾ä¸­ç§»é™¤å¤šé‡å…±çº¿æ€§ä¸¥é‡çš„ç‰¹å¾
        selected_features = [f for f in high_correlation_features if f not in high_vif_features]
        
        # ç¡®ä¿æ‰€æœ‰ç»Ÿè®¡æ˜¾è‘—çš„ç‰¹å¾éƒ½è¢«åŒ…å«
        for feature in significant_features:
            if feature not in selected_features and feature != target_col:
                selected_features.append(feature)
        
        # ç¡®ä¿ç›®æ ‡å˜é‡åœ¨ç‰¹å¾é›†ä¸­
        if target_col not in selected_features:
            selected_features.append(target_col)
        
        st.success(f"ç‰¹å¾é€‰æ‹©å®Œæˆï¼ä» {len(numeric_cols)} ä¸ªç‰¹å¾ä¸­é€‰å‡º {len(selected_features)} ä¸ªç‰¹å¾")
        
        return selected_features
    
    except Exception as e:
        st.error(f"ç‰¹å¾é€‰æ‹©è¿‡ç¨‹ä¸­å‘ç”Ÿé”™è¯¯: {str(e)}")
        import traceback
        st.code(traceback.format_exc())
        # è¿”å›é»˜è®¤ç‰¹å¾ä½œä¸ºå¤‡é€‰
        if 'Close' in df.columns:
            return ['Close'] + [col for col in df.columns if col != 'Close'][:5]  # è¿”å›Closeå’Œå…¶ä»–5ä¸ªç‰¹å¾
        else:
            return df.columns.tolist()[:6]  # è¿”å›å‰6ä¸ªç‰¹å¾ 