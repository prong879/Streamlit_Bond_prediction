\
// ... existing code ...
            2.2.1.2. ACF与PACF分析
            2.2.1.3. 模型定阶与参数估计

        2.2.2. LSTM模型原理 (作为RNN的代表性变体)

            LSTM (Long Short-Term Memory) 是循环神经网络 (RNN) 的一种特殊变体，旨在解决标准RNN在处理长序列时容易出现的梯度消失或梯度爆炸问题[10]。

            2.2.2.1. RNN与长短期记忆网络结构

            传统的RNN通过在隐藏层引入循环连接，使得网络能够记忆先前时间步的信息，从而处理序列数据。然而，这种简单的循环结构在学习长期依赖关系时效果不佳。
            LSTM通过引入精心设计的门控机制（输入门、遗忘门、输出门）来控制信息在网络中的流动和存留。这些门结构使得LSTM单元能够选择性地记忆、遗忘或输出信息，从而有效地捕捉时间序列中的长期依赖关系。一个基本的LSTM单元通常包含一个细胞状态 (cell state) 和三个门：
            *   **遗忘门 (Forget Gate):** 决定从细胞状态中丢弃哪些信息。
            *   **输入门 (Input Gate):** 决定哪些新的信息会被存放在细胞状态中。
            *   **输出门 (Output Gate):** 决定当前细胞状态的哪些部分会作为当前时间步的输出。

            2.2.2.2. LSTM在时序预测中的应用

            由于LSTM能够有效处理长期依赖问题，它被广泛应用于各种时序预测任务，包括股票价格预测、天气预报、语音识别等。在金融领域，LSTM常用于预测股票收益率、市场波动性等，其能够从历史价格序列、交易量甚至文本数据中学习复杂的模式[17, 18, 26, 27]。LSTM网络的优势在于可以记住网络中的短期和长期值，因此被广泛应用于序列数据分析。

            2.2.2.3. 序列数据处理与归一化

            在使用LSTM模型处理时序数据时，通常需要进行预处理步骤。这包括将时间序列数据转换为监督学习问题所需的输入序列 (X) 和输出值 (y) 的格式，例如使用滑动窗口方法创建固定长度的输入序列来预测下一个或未来几个时间点的值。
            此外，由于神经网络对输入数据的尺度敏感，通常需要对输入特征进行归一化（例如，使用Min-Max Scaling或Standardization），将数据缩放到一个较小的区间（如0到1或-1到1），这有助于加速模型训练和提高模型性能。

        2.2.3. (可选) Prophet模型简介
        (如果项目中实际使用了Prophet模型，此处应进行介绍)

        2.2.4. BP神经网络
            2.2.4.1. 模型结构与原理
            BP (Backpropagation) 神经网络是一种按误差逆向传播算法训练的多层前馈网络。它通常由输入层、一个或多个隐藏层和输出层组成。其学习过程包括两个阶段：前向传播和误差反向传播。在前向传播阶段，输入信号通过隐藏层处理后到达输出层，产生输出结果。然后，计算输出结果与真实值之间的误差。在反向传播阶段，该误差会从输出层逐层向后传播，并用于调整网络中各连接的权重，目标是最小化网络的总误差。BP网络通过梯度下降法等优化算法迭代更新权重[10]。

            2.2.4.2. 优缺点与金融应用局限
            BP神经网络的优点在于其能够学习和存储大量的输入输出模式映射关系，具有较强的非线性建模能力。然而，它也存在一些缺点：训练收敛速度可能较慢，容易陷入局部最小值，网络结构（如隐藏层数和神经元数量）的选择缺乏理论指导，需要经验调试。此外，BP神经网络没有自反馈调节机制，不具备记忆功能，因此在直接处理时间序列数据，特别是捕捉时间依赖性方面表现不佳，其对金融时序数据的预测精度有待进一步提高[10]。

        2.2.5. 卷积神经网络 (CNN)
            2.2.5.1. 模型结构与原理
            卷积神经网络 (Convolutional Neural Network, CNN) 是一种受人类视觉机制启发的深度学习模型，最初主要用于图像处理领域。CNN的核心组成部分包括卷积层、池化层和全连接层。
            *   **卷积层 (Convolutional Layer):** 通过使用可学习的滤波器（或称为卷积核）对输入数据进行卷积操作，提取局部特征。不同的滤波器可以捕捉输入数据的不同方面特征。
            *   **池化层 (Pooling Layer):** 通常在卷积层之后，用于降低特征图的空间维度，减少计算量，并增强模型的平移不变性。常用的池化操作有最大池化 (Max Pooling) 和平均池化 (Average Pooling)。
            *   **全连接层 (Fully Connected Layer):** 在经过多层卷积和池化操作后，将提取到的高级特征展平，并通过一个或多个全连接层进行分类或回归。
            CNN通过权值共享（卷积核在整个输入上共享参数）和局部感受野（神经元只对输入数据的局部区域敏感）来有效减少模型参数数量并提取空间层次特征。

            2.2.5.2. 优缺点与金融应用特点
            CNN的优点包括能够自动从原始数据中学习和提取有用的特征，特别擅长捕捉局部模式和空间结构信息。权值共享机制使得模型更易于训练且具有一定的平移不变性。在金融领域，CNN有时被用于从高频交易数据、K线图（视为图像）或金融新闻文本中提取特征，用于预测市场趋势或波动性[20, 22, 24]。例如，将一维的金融时间序列数据（如股价序列）视为一维图像，CNN可以从中提取价格模式。
            然而，CNN的传统结构主要针对网格状数据（如图像），在直接处理非结构化或纯粹的时间序列数据时，其捕捉长期时间依赖关系的能力可能不如RNN或LSTM。此外，池化操作可能会丢失一些有用的信息，且模型的内部工作机制有时缺乏直观解释性（“黑盒”问题）。

        2.2.6. 循环神经网络 (RNN)
            2.2.6.1. 模型结构与原理
            循环神经网络 (Recurrent Neural Network, RNN) 是一类专门用于处理序列数据的神经网络。与前馈神经网络不同，RNN在其结构中引入了循环连接，使得信息可以在网络内部持久存在。在每个时间步，RNN接收当前输入和前一个时间步的隐藏状态，然后计算当前的输出和新的隐藏状态。这个隐藏状态可以被看作是网络对先前序列信息的“记忆”。
            RNN的特殊结构使其能够处理任意长度的序列输入，并在处理序列数据时考虑到每个时间步的前后信息。由于在所有时间步共享相同的参数（权重），RNN的参数数量相对较少，有助于降低模型复杂度。

            2.2.6.2. 优缺点与在序列数据处理中的核心作用
            RNN的核心优势在于其能够对序列数据进行建模，捕捉时间依赖性。这使其成为自然语言处理、语音识别、时间序列预测等任务的理想选择。
            然而，标准RNN在处理长序列时面临显著的挑战，即梯度消失或梯度爆炸问题。梯度消失使得网络难以学习到序列中较早时间步的信息（长期依赖），而梯度爆炸则会导致训练不稳定。此外，标准RNN的记忆能力相对有限。尽管如此，RNN作为理解更复杂序列模型（如LSTM和GRU）的基础，其核心的循环和状态传递思想对于序列数据处理至关重要。

        2.2.7. Transformer模型
            2.2.7.1. 模型结构与自注意力机制
            Transformer模型最初是为机器翻译任务设计的，但其强大的序列处理能力使其迅速扩展到其他领域，包括时间序列预测[12]。Transformer的核心创新在于完全抛弃了RNN的循环结构，而是依赖于一种称为“自注意力机制” (Self-attention) 的模块。
            自注意力机制允许模型在处理序列中的每个元素时，权衡序列中所有其他元素对当前元素的重要性。它通过计算查询 (Query)、键 (Key) 和值 (Value) 向量来实现这一点，并根据查询和键之间的相似性为值分配权重。多头注意力 (Multi-head Attention) 机制则允许模型在不同的表示子空间中并行地关注来自不同位置的信息。
            一个典型的Transformer模型由编码器栈 (Encoder stack) 和解码器栈 (Decoder stack) 组成。每个编码器包含一个多头自注意力模块和一个前馈神经网络。每个解码器除了这两个模块外，还插入了一个编码器-解码器注意力模块，使其能够关注编码器的输出。位置编码 (Positional Encoding) 被添加到输入嵌入中，以向模型提供关于序列中元素位置的信息，因为自注意力机制本身不处理序列顺序。

            2.2.7.2. 优缺点与金融应用潜力
            Transformer的优点在于其强大的并行计算能力（由于没有循环依赖）和捕捉长距离依赖关系的能力。自注意力机制使其能够直接建模序列中任意两个位置之间的关系，而无需像RNN那样通过多个时间步传递信息。这使其在处理长序列数据时表现出色[13]。
            缺点方面，Transformer模型通常需要大量的训练数据和计算资源。其对输入数据的顺序不敏感（需要显式的位置编码），并且模型的解释性有时不如RNN直接。此外，对于某些类型的时间序列，特别是噪声较大的金融时间序列，Transformer的性能可能对超参数和数据预处理非常敏感。
            尽管如此，Transformer在金融时间序列预测领域显示出巨大潜力，特别是在处理包含多种影响因素、具有复杂依赖关系的金融数据时[19, 25]。例如，它可以用于预测股票价格、分析市场情绪（结合NLP）等。

        2.2.8. 深度学习在股价预测中的应用概述
            金融市场价格预测，特别是股价预测，因其高噪声、非线性和复杂动态性而极具挑战性。传统的有效市场假说(EMH)认为市场价格已反映所有可用信息，技术分析和基本面分析无效[14]。然而，许多研究通过计量模型（如AR、ARCH[15]）和机器学习方法对市场有效性提出质疑。

            2.2.8.1. 单一深度学习模型的应用
            深度学习模型凭借其强大的非线性建模能力，为股价预测提供了新的途径。RNN及其变体（如LSTM和GRU）因其固有的序列处理能力成为首选。例如，研究者采用优化的RNN预测股价[17]，或结合LSTM与全连接网络对中国股市进行预测，通过多目标训练获得更好效果[18]。章宁等人[19]使用Transformer模型进行预测，并与LSTM、SVR对比，在预期超额收益率预测上取得良好效果。

            2.2.8.2. 混合深度学习模型的探索
            单一深度学习模型各有优缺点（如DMLP易陷入局部最小，LSTM计算耗时）。因此，研究者尝试融合多个模型以结合各自优势，提升性能。集成学习方法通常优于单一模型。例如，Zhou等人[20]结合LSTM和CNN进行对抗性训练预测高频股市。谷歌的BERT模型[21]（一种双向Transformer框架）也启发了金融领域的应用。Lu等人[22]结合CNN、BiLSTM与注意力机制预测上证指数。Darapaneni等人[23]混合LSTM与随机森林，利用历史价格和情绪数据预测股价。Shi等人[24]提出基于注意力机制的CNN-LSTM和XGBoost混合模型。危冰淋等人[25]提出的Transformer-LSTM模型在碳排放权交易价格预测中表现优于单一模型。

        2.2.9. 深度学习在投资组合决策中的应用概述
            Markowitz提出的均值方差模型[5, 6, 7]证明了通过构建投资组合、分散化投资以在相同风险水平下实现更高期望收益率的可行性。然而，该模型对输入参数（尤其是预期超额收益率）的准确性极为敏感，预测误差会导致投资组合绩效劣化[8, 9]。
            深度学习通过提高资产收益率的预测准确度，为提升投资组合绩效提供了新工具。研究人员尝试将LSTM、Transformer等模型应用于投资组合构建。例如，LEE等[26]使用RNN、LSTM和GRU预测股票收益，并据此构建基于阈值的投资组合。Wang等[27]结合LSTM和Markowitz均值方差模型构建投资决策模型，并强调了资产预选的重要性。张虎等人[29]运用自注意力神经网络预测股价趋势，筛选股票构建组合。闫洪举[30]使用深度自动编码器确定指数跟踪组合的成分股，并用深度神经网络测算权重。章宁[19]基于Transformer模型的预测结果，通过传统投资组合模型构建组合，并证明其相对于LSTM和SVR模型的绩效提升。刘祺等人[28]构建了复杂的ARIMA-GARCH-CNN-BiLSTM-AT-XGBoost混合模型，并结合Bayes方法进行投资组合，取得了较优绩效。
            这些研究表明，深度学习在因子选股、量化投资、动态调整组合权重等方面具有巨大潜力。

    2.3. 模型评估指标
// ... existing code ...
